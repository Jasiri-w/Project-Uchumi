{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff88c15-e542-4398-921a-2b85a23960f8",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecb6dd3-8e8c-4cec-89e3-6526f929fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 18:49:05.261658: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-03 18:49:06.301013: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-03 18:49:06.709099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-03 18:49:07.356128: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-03 18:49:07.360157: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-03 18:49:08.457085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-03 18:49:11.954760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "import emoji\n",
    "import gensim.downloader as api\n",
    "# Load pre-trained word vectors\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7ea9a-c8d5-4b69-8ecf-4fddfd0bcfbc",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. Lowercasing\n",
    "2. Tokenization\n",
    "3. Stopword removal\n",
    "4. Stemming/Lemmatization\n",
    "5. Language Detection\n",
    "6. Removal of emojis if applicable\n",
    "7. Negation Handling\n",
    "8. Word Embeddings\n",
    "9. Padding and Truncation\n",
    "10. Data splitting: Training, Validation & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52d942-9321-4335-ab8f-301ee5a94904",
   "metadata": {},
   "source": [
    "## Lowercasing, Tokenization, Stopword Removal, Lemmatization, Negation Handling & Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5b0f8f-27bd-409f-a28c-723976e06b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              article  \\\n",
      "0   A disagreement between two widows in Kakamega ...   \n",
      "1   Meru County government is yet to pass its 2024...   \n",
      "2   What you need to know:\\n- At least 23 youths w...   \n",
      "3   What you need to know:\\n- Residents feel autho...   \n",
      "4   What you need to know:\\n- Dish out money and g...   \n",
      "5   Hi Pastor,\\nMy step dad married my biological ...   \n",
      "6   What you need to know:\\n- Gen Zs' gamophobia —...   \n",
      "7   What you need to know:\\n- Irrespective of octa...   \n",
      "8   What you need to know:\\n- Self-care is wholeso...   \n",
      "9   Somali President Hassan Sheikh Mohamud came to...   \n",
      "10  The recent Kenyan protests are a warning that ...   \n",
      "11  Trapped in a Catholic mission sheltering dozen...   \n",
      "12  What you need to know:\\n- In Nairobi on Tuesda...   \n",
      "13  What you need to know:\\n- Learning resumed on ...   \n",
      "14  What you need to know:\\n- Despite the normalis...   \n",
      "15  What you need to know:\\n- I had my first ever ...   \n",
      "16  What you need to know:\\n- Boda boda riders, on...   \n",
      "17  President William Ruto's establishment of a Na...   \n",
      "18  Interior Cabinet Secretary Kithure Kindiki has...   \n",
      "19  National Assembly Speaker Moses Wetang’ula is ...   \n",
      "20  What you need to know:\\n- Despite the High Cou...   \n",
      "21  Six politicians, a businessman and two NGOs ar...   \n",
      "22  They have become so brazen that they aim their...   \n",
      "23  Multiple world and Olympics 3,000m steeplechas...   \n",
      "24  Volleyball Olympian Gaudencia Makokha was bare...   \n",
      "25  Oil and gold markets on edge ahead of key econ...   \n",
      "26  The construction industry is a significant con...   \n",
      "27  “Through a local partnership in the DRC from M...   \n",
      "28  “Due to the planned demonstrations Tuesday Jul...   \n",
      "29  Nicotine use can be traced back to 12,300 year...   \n",
      "30  The Treasury faces pressure to raise at least ...   \n",
      "31  Tier I banks NCBA Group and Stanbic Bank Kenya...   \n",
      "32  This is a proposed law that reveals how revenu...   \n",
      "33  Earlier this year, an old story about former N...   \n",
      "34  The Supreme Court has declined to hear a dispu...   \n",
      "35  Kenya is witnessing a wave of national outrage...   \n",
      "36  The US has warned that Kenya’s rising debt bur...   \n",
      "37  The Nairobi Securities Exchange (NSE) posted t...   \n",
      "38  Last year’s momentum is, however, likely to be...   \n",
      "39  Equity Group CEO James Mwangi has rejected his...   \n",
      "40  The Nairobi Securities Exchange (NSE) has shed...   \n",
      "41  “Being chosen for such a long and important ev...   \n",
      "42  The Kenya Airports Authority (KAA) is going ha...   \n",
      "43  Dolly Sagwe, a financial wellness coach and th...   \n",
      "44  Mwananchi Credit has failed in its bid to lift...   \n",
      "45  Packaging solutions provider, Shri Krishana Ov...   \n",
      "46  NCBA Group has announced the completion of the...   \n",
      "47  A consignment of 564 tonnes of fertiliser dona...   \n",
      "48  The Kenyan government has delayed lowering Nai...   \n",
      "49  Despite earlier directing the Treasury to cut ...   \n",
      "\n",
      "                                      cleaned_article  \\\n",
      "0   disagreement two widow kakamega bury remains h...   \n",
      "1   meru county government yet pas budget standoff...   \n",
      "2   need know least youth shot tuesday protest nak...   \n",
      "3   need know resident feel authority playing favo...   \n",
      "4   need know dish money goody drop dime surround ...   \n",
      "5   hi pastor step dad married biological mum seve...   \n",
      "6   need know gen z gamophobia fear commitment fea...   \n",
      "7   need know irrespective octane rating engine al...   \n",
      "8   need know selfcare wholesome involving physica...   \n",
      "9   somali president hassan sheikh mohamud came po...   \n",
      "10  recent kenyan protest warning international mo...   \n",
      "11  trapped catholic mission sheltering dozen woma...   \n",
      "12  need know nairobi tuesday trader armed wooden ...   \n",
      "13  need know learning resumed monday midterm brea...   \n",
      "14  need know despite normalisation menstrual cycl...   \n",
      "15  need know first ever trip uganda last week alw...   \n",
      "16  need know boda boda rider accused sexual haras...   \n",
      "17  president william rutos establishment national...   \n",
      "18  interior cabinet secretary kithure kindiki war...   \n",
      "19  national assembly speaker moses wetangula man ...   \n",
      "20  need know despite high court banning use tearg...   \n",
      "21  six politician businessman two ngo radar state...   \n",
      "22  become brazen aim gun directly kenyan sometime...   \n",
      "23  multiple world olympics steeplechase champion ...   \n",
      "24  volleyball olympian gaudencia makokha barely f...   \n",
      "25  oil gold market edge ahead key economic releas...   \n",
      "26  construction industry significant contributor ...   \n",
      "27  local partnership drc march commenced distribu...   \n",
      "28  due planned demonstration tuesday july school ...   \n",
      "29  nicotine use traced back year ago global tobac...   \n",
      "30  treasury face pressure raise least extra sh bi...   \n",
      "31  tier bank ncba group stanbic bank kenya borrow...   \n",
      "32  proposed law reveals revenue raised tax fine s...   \n",
      "33  earlier year old story former nintendo ceo sat...   \n",
      "34  supreme court declined hear dispute national b...   \n",
      "35  kenya witnessing wave national outrage riot cl...   \n",
      "36  u warned kenya rising debt burden undermining ...   \n",
      "37  nairobi security exchange nse posted highest r...   \n",
      "38  last year momentum however likely hit ongoing ...   \n",
      "39  equity group ceo james mwangi rejected bonus r...   \n",
      "40  nairobi security exchange nse shed sh billion ...   \n",
      "41  chosen long important event really humbling sa...   \n",
      "42  kenya airport authority kaa going hard kenya a...   \n",
      "43  dolly sagwe financial wellness coach cofounder...   \n",
      "44  mwananchi credit failed bid lift court order b...   \n",
      "45  packaging solution provider shri krishana over...   \n",
      "46  ncba group announced completion acquisition pe...   \n",
      "47  consignment tonne fertiliser donated kenya rus...   \n",
      "48  kenyan government delayed lowering nairobi exp...   \n",
      "49  despite earlier directing treasury cut kenya t...   \n",
      "\n",
      "                                         word_vectors  \n",
      "0   [[0.23523, -0.056734, 0.44165, 0.68045, -0.560...  \n",
      "1   [[-0.3391, 0.22768, -0.28521, 0.21346, -0.3290...  \n",
      "2   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "3   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "4   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "5   [[0.1444, 0.23979, 0.96693, 0.31629, -0.36064,...  \n",
      "6   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "7   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "8   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "9   [[-0.48921, 0.31982, 0.33619, -0.44536, 0.6854...  \n",
      "10  [[-0.18891, 0.28762, -0.1152, 0.00025325, 0.11...  \n",
      "11  [[-0.18524, 0.44675, 0.28606, -0.16169, -0.113...  \n",
      "12  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "13  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "14  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "15  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "16  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "17  [[-0.064549, -0.13812, 0.50017, 0.41434, 0.458...  \n",
      "18  [[-0.97483, -0.45512, 0.4124, 0.24541, 0.88572...  \n",
      "19  [[-0.0033138, 0.38946, 0.2635, -0.29199, 0.380...  \n",
      "20  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "21  [[0.1868, 0.48615, 0.14036, -0.22788, 0.36962,...  \n",
      "22  [[-0.23273, -0.089062, 0.49115, -0.49883, 0.46...  \n",
      "23  [[-0.16573, 0.54371, -0.62806, 0.44953, 0.8690...  \n",
      "24  [[0.69795, 0.4813, 0.54966, -0.16301, -0.70516...  \n",
      "25  [[0.25654, 1.1541, 0.1682, -0.34945, 0.82621, ...  \n",
      "26  [[0.5598, 0.40884, -0.86392, 0.44137, 0.094008...  \n",
      "27  [[-0.25705, -0.056895, -0.38103, -0.1633, -0.1...  \n",
      "28  [[-0.38091, -0.39921, 0.36298, 0.38955, -0.471...  \n",
      "29  [[0.82655, -1.0851, 0.22304, 0.0047817, -0.369...  \n",
      "30  [[0.39275, -0.27413, 0.70834, 0.1314, 0.31045,...  \n",
      "31  [[-0.35529, 0.43278, 0.59188, 0.42711, -0.1606...  \n",
      "32  [[0.042485, -0.0069038, 0.0090882, 0.65083, 0....  \n",
      "33  [[0.086955, -0.13645, 0.23288, 0.14583, 0.0053...  \n",
      "34  [[0.85622, -0.30057, 0.98826, 0.42044, 0.54632...  \n",
      "35  [[-0.36526, 0.48584, -0.19377, -0.13632, 0.484...  \n",
      "36  [[-0.031087, 0.22155, 0.44494, 0.92176, -0.186...  \n",
      "37  [[-0.29677, -0.031959, 0.021792, -0.39868, 0.2...  \n",
      "38  [[0.23745, 0.1241, 0.6252, -0.46639, 0.12238, ...  \n",
      "39  [[0.50174, 0.64254, -0.08661, 0.09809, -0.2457...  \n",
      "40  [[-0.29677, -0.031959, 0.021792, -0.39868, 0.2...  \n",
      "41  [[-0.22171, 0.27791, 0.18555, -0.013296, 0.745...  \n",
      "42  [[-0.36526, 0.48584, -0.19377, -0.13632, 0.484...  \n",
      "43  [[0.57637, -0.62092, 0.51566, -0.79249, -0.029...  \n",
      "44  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
      "45  [[-0.27871, 0.19927, -0.040328, -0.47912, 0.42...  \n",
      "46  [[0.24623, -0.20167, -0.07961, -0.050482, -0.1...  \n",
      "47  [[0.36528, 0.91986, -0.072429, -0.26882, 0.146...  \n",
      "48  [[0.0052971, 0.18642, 0.14117, -0.068971, 0.14...  \n",
      "49  [[-0.058478, 0.19373, 0.3631, -0.19584, 0.0260...  \n"
     ]
    }
   ],
   "source": [
    "# Load existing data from CSV\n",
    "try:\n",
    "    df = pd.read_csv('/workspaces/Project-Uchumi/data/raw/articles.csv')\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame(columns=['url', 'article', 'date'])\n",
    "\n",
    "# Language detection for providence of Swahili data\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "df['language'] = df['article'].apply(detect_language)\n",
    "\n",
    "# Define stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "swahili_stopwords = [\n",
    "    \"na\", \"za\", \"kwa\", \"ya\", \"ndani\", \"je\", \"ni\", \"hata\", \"pia\", \"au\", \"wakati\",\n",
    "    \"hivyo\", \"nini\", \"kama\", \"bila\", \"kisha\", \"sasa\", \"yake\", \"yao\", \"hizo\",\n",
    "    \"zao\", \"yao\", \"yenu\", \"yake\", \"zake\", \"lakini\", \"au\", \"nao\", \"wao\",\n",
    "    \"yao\", \"yake\", \"kwenda\", \"kuwa\", \"kuwa\", \"wao\", \"naye\", \"ninyi\",\n",
    "    \"huku\", \"yako\", \"basi\", \"kabla\", \"kutoka\", \"katika\", \"mimi\",\n",
    "    \"yako\", \"kweli\", \"kabisa\", \"hasa\", \"hapo\", \"hata\", \"hivyo\",\n",
    "    \"mbali\", \"mara\", \"zaidi\", \"karibu\", \"kila\", \"mmoja\", \"mwingine\",\n",
    "    \"nyingine\", \"wengine\", \"yoyote\", \"wote\", \"huyo\", \"huo\", \"kwamba\",\n",
    "    \"lakini\", \"mbali\", \"mimi\", \"mmoja\", \"muda\", \"mwenyewe\", \"naam\",\n",
    "    \"pamoja\", \"sana\", \"sasa\", \"sisi\", \"vile\", \"wa\", \"wakati\", \"wake\",\n",
    "    \"wakiwa\", \"wana\", \"wao\", \"watu\", \"wengine\", \"wote\", \"ya\", \"yake\",\n",
    "    \"yangu\", \"yao\", \"yeye\", \"yule\", \"za\", \"zaidi\", \"zake\"\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'text' was the input text\n",
    "# stopwords_removed_text = ' '.join([word for word in text.split() if word.lower() not in swahili_stopwords])\n",
    "\n",
    "\n",
    "# Define stemmer and lemmatizer for nltk.stem\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Negation handling for sentiment analysis\n",
    "negation_words = [\"no\", \"not\", \"never\", \"none\", \"nobody\", \"nowhere\", \"nothing\", \"neither\", \"nor\", \"cannot\", \"can't\", \"won't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\"]\n",
    "\n",
    "# Define the <UNK> token\n",
    "UNK_TOKEN = '<UNK>'\n",
    "UNK_VECTOR = np.zeros_like(word_vectors.get_vector(word_vectors.index_to_key[0]))\n",
    "\n",
    "# Add the <UNK> token to your word vectors\n",
    "word_vectors[UNK_TOKEN] = UNK_VECTOR\n",
    "\n",
    "# Function for preprocessing\n",
    "def process_tokens(text, language):\n",
    "    # Remove special characters, numbers, and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform stemming or lemmatization\n",
    "    if language == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in english_stopwords]\n",
    "    elif language == 'sw':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in swahili_stopwords]\n",
    "\n",
    "    negated = False\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            negated = not negated\n",
    "        elif negated:\n",
    "            processed_tokens.append('NOT_' + token)\n",
    "        else:\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def get_word_embeddings(processed_tokens):\n",
    "    # Get word embeddings for each token\n",
    "    embeddings = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_vectors:\n",
    "            embeddings.append(word_vectors[token])\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            embeddings.append(word_vectors['<UNK>'])  # Use a special token for unknown words\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "    # Reconstruct the text\n",
    "\n",
    "def preprocess_text(text, language):\n",
    "    processed_tokens = process_tokens(text, language)\n",
    "    cleaned_article = ' '.join(processed_tokens)\n",
    "    word_vectors = get_word_embeddings(processed_tokens)\n",
    "\n",
    "    return cleaned_article, word_vectors\n",
    "\n",
    "\n",
    "# Apply preprocessing to each article\n",
    "df['cleaned_article'], df['word_vectors'] = zip(*df.apply(lambda row: preprocess_text(row['article'], row['language']), axis=1))\n",
    "\n",
    "# Print the preprocessed data\n",
    "print(df[['article', 'cleaned_article', 'word_vectors']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821e7fa-40d5-4aa7-9285-8e508e43bfcd",
   "metadata": {},
   "source": [
    "## Emoji removal (Ignore for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9724c2-4989-4e37-a880-c807c84f7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! \n"
     ]
    }
   ],
   "source": [
    "# Emoji Removal for future if applicable in say tweets or comments or Linkedin Posts\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Example usage\n",
    "text_with_emojis = \"Hello! 😀🚀🌟\"\n",
    "text_without_emojis = remove_emojis(text_with_emojis)\n",
    "print(text_without_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14eefa-b90a-4673-b383-03599e046d27",
   "metadata": {},
   "source": [
    "## Padding & Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6435288-4876-446c-8378-c5446c04d426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  1, -1],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the maximum sequence length\n",
    "max_length = 1000\n",
    "# Pad and truncate the sequences\n",
    "padded_sequences = pad_sequences(df['word_vectors'], maxlen=max_length, padding='post', truncating='post')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31f6ac-5659-4c97-9d56-94b7dea43a6d",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a9061d-0ad2-45ae-9ea1-9e0c36fa4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/validation and test sets\n",
    "X_train_val, X_test = train_test_split(padded_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training/validation set into training and validation sets\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b6c282-2e47-43c8-87ca-4ad858750b92",
   "metadata": {},
   "source": [
    "## Export Pre-processed Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a921265a-3648-4bfb-8aa8-8b94a7c3c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/workspaces/Project-Uchumi/data/processed/preprocessed_articles.csv\", index=False, mode='a', header=not os.path.exists(\"/workspaces/Project-Uchumi/data/processed/preprocessed_articles.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efd72bc-7094-47ac-8de1-dad9bc89436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     cleaned_article  dominant_topic  \\\n",
      "0  disagreement two widow kakamega bury remains h...               4   \n",
      "\n",
      "   topic_probability  \n",
      "0           0.997012  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Sample DataFrame (assuming `df` is your DataFrame)\n",
    "df1 = pd.DataFrame({\n",
    "    'cleaned_article': [\"disagreement two widow kakamega bury remains husband fuelled tension family resulting death injury destruction property burial late joseph otipa osundwa died april scheduled june mumias senior resident magistrate marcella onyango issued order june body collected mortuary st mary mission hospital mumias friday june taken first wife home burial saturday per court order family first wife selpha maende buried mr otipas body home accordance luhya custom court also ordered burial cost borne deceased two wife child relative mortuary bill would shared equally widow family dispute direct party bear cost mortuary expense shared equally directed m onyango however family second wife margaret otipa furious court decision grant burial right first family claimed deceased died home spent time life father living mother year unfair court give body stepmother bury citing luhya custom tradition said one son day burial hell broke loose first family went burial site bury remains deceased margaret child interrupted procession sparking violent confrontation coffin containing mr otipas remains suddenly thrown ground pallbearer ran safety ensuing clash resulted death yearold mohammed anyanga brother deceased mr anyanga attacked one nephew trying separate warring party rushed kakamega county general hospital ct scan revealed suffered fractured skull blood clot brain later died undergoing treatment speaking father death son patrick anyanga said life would NOT_two NOT_family NOT_father NOT_brother NOT_deceased NOT_every NOT_right NOT_intervene NOT_bring NOT_peace NOT_warring NOT_family NOT_wrong NOT_young NOT_man NOT_knew NOT_uncle NOT_raise NOT_panga NOT_hack NOT_death NOT_lamented NOT_mumias NOT_east NOT_subcounty NOT_police NOT_commander NOT_doris NOT_chemosi NOT_confirmed NOT_incident NOT_said NOT_investigation NOT_begun NOT_detective NOT_already NOT_questioning NOT_recording NOT_statement NOT_attended NOT_burial NOT_ceremony NOT_witnessed NOT_deadly NOT_clash NOT_expect NOT_make NOT_arrest NOT_charge NOT_people NOT_killing NOT_old NOT_man NOT_soon NOT_complete NOT_investigation NOT_m NOT_chemosi NOT_said NOT_body NOT_deceased NOT_taken NOT_st NOT_mary NOT_mission NOT_hospital NOT_mortuary NOT_expected NOT_remain NOT_warring NOT_wife NOT_agree NOT_burial NOT_site\"]\n",
    "})\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(df1['cleaned_article'])\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Assign topics to articles\n",
    "topic_results = lda.transform(X)\n",
    "df1['dominant_topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# Topic probabilities\n",
    "df1['topic_probability'] = topic_results.max(axis=1)\n",
    "\n",
    "print(df1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
