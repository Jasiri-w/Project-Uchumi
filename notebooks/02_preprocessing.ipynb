{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff88c15-e542-4398-921a-2b85a23960f8",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ecb6dd3-8e8c-4cec-89e3-6526f929fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "import emoji\n",
    "import gensim.downloader as api\n",
    "# Load pre-trained word vectors\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7ea9a-c8d5-4b69-8ecf-4fddfd0bcfbc",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. Lowercasing\n",
    "2. Tokenization\n",
    "3. Stopword removal\n",
    "4. Stemming/Lemmatization\n",
    "5. Language Detection\n",
    "6. Removal of emojis if applicable\n",
    "7. Negation Handling\n",
    "8. Word Embeddings\n",
    "9. Padding and Truncation\n",
    "10. Data splitting: Training, Validation & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52d942-9321-4335-ab8f-301ee5a94904",
   "metadata": {},
   "source": [
    "## Lowercasing, Tokenization, Stopword Removal, Lemmatization, Negation Handling & Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5b0f8f-27bd-409f-a28c-723976e06b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              article  \\\n",
      "0   Hopes by Kakamega Deputy Governor Ayub Savula ...   \n",
      "1   What you need to know:\\n- Only a few individua...   \n",
      "2   What you need to know:\\n- According to Makueni...   \n",
      "3   One of the demonstrators who allegedly entered...   \n",
      "4   What you need to know:\\n- Kasaine was reported...   \n",
      "5   The National Police Service (NPS) has refuted ...   \n",
      "6   What you need to know:\\n- The groups said if t...   \n",
      "7   What you need to know:\\n- The scheme covers na...   \n",
      "8   Complexities of Finance Bills have primarily b...   \n",
      "9   The Finance Bill, 2024 has elicited anger amon...   \n",
      "10  When the history of the Duruma community is fi...   \n",
      "11  What you need to know:\\n- Widows in Africa cou...   \n",
      "12  Mr Gachagua, however, says the issues between ...   \n",
      "13  What you need to know:\\n- The Deputy President...   \n",
      "14  What you need to know:\\n- Omtatah argues that ...   \n",
      "15  What you need to know:\\n- Khat is also cultiva...   \n",
      "16  What you need to know:\\n- Trump hailed the rul...   \n",
      "17  Kenya and the European Union have enforced a t...   \n",
      "18  Tanzaniaâ€™s seaport operations are set for a ma...   \n",
      "19  Kenyan taxpayers paid $1.53 billion (Sh197.4 b...   \n",
      "20  President William Ruto faces public scrutiny a...   \n",
      "21  The Merriam-Webster Dictionary defines gasligh...   \n",
      "22  \"The NHIF contract was executed for a period o...   \n",
      "23  The demand for Treasury bills (T-bills) by inv...   \n",
      "24  At the height of the anti-Finance Bill protest...   \n",
      "25  I fondly remember an opportunity in telecommun...   \n",
      "26  Kenyans issued fewer money orders last year du...   \n",
      "27  Less than three percent of jobs at ministries,...   \n",
      "28  The National Housing Corporation (NHC) is stuc...   \n",
      "29  East African countries have achieved 100 perce...   \n",
      "30  Kenya has recently emerged as a beacon of civi...   \n",
      "\n",
      "                                      cleaned_article  \\\n",
      "0   hope kakamega deputy governor ayub savula two ...   \n",
      "1   need know individual institution lamu privileg...   \n",
      "2   need know according makueni finance executive ...   \n",
      "3   one demonstrator allegedly entered parliament ...   \n",
      "4   need know kasaine reportedly shot dead metre g...   \n",
      "5   national police service np refuted claim seven...   \n",
      "6   need know group said government us decisive ex...   \n",
      "7   need know scheme cover national government emp...   \n",
      "8   complexity finance bill primarily concern olde...   \n",
      "9   finance bill elicited anger among kenyan thous...   \n",
      "10  history duruma community finally written great...   \n",
      "11  need know widow africa could soon reason smile...   \n",
      "12  mr gachagua however say issue president addres...   \n",
      "13  need know deputy president made revelation chu...   \n",
      "14  need know omtatah argues exclusion senate unde...   \n",
      "15  need know khat also cultivated country includi...   \n",
      "16  need know trump hailed ruling social medium po...   \n",
      "17  kenya european union enforced trade deal decad...   \n",
      "18  tanzania seaport operation set major overhaul ...   \n",
      "19  kenyan taxpayer paid billion sh billion intere...   \n",
      "20  president william ruto face public scrutiny de...   \n",
      "21  merriamwebster dictionary defines gaslighting ...   \n",
      "22  nhif contract executed period one year july ju...   \n",
      "23  demand treasury bill tbills investor dried eve...   \n",
      "24  height antifinance bill protest organiser shat...   \n",
      "25  fondly remember opportunity telecommunication ...   \n",
      "26  kenyan issued fewer money order last year due ...   \n",
      "27  le three percent job ministry department agenc...   \n",
      "28  national housing corporation nhc stuck hundred...   \n",
      "29  east african country achieved percent mobile p...   \n",
      "30  kenya recently emerged beacon civic political ...   \n",
      "\n",
      "                                         word_vectors  \n",
      "0   [[0.034877, 0.46079, 0.068392, -0.028982, 0.16...  \n",
      "1   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "2   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "3   [[-0.22557, 0.49418, 0.4861, -0.4332, 0.13738,...  \n",
      "4   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "5   [[-0.0033138, 0.38946, 0.2635, -0.29199, 0.380...  \n",
      "6   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "7   [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "8   [[-0.56268, 0.29065, 0.64271, 1.0655, -0.35361...  \n",
      "9   [[-0.60946, -0.21094, 0.050396, -0.23405, 0.39...  \n",
      "10  [[0.2146, 0.73921, 0.79858, 0.10209, 0.10226, ...  \n",
      "11  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "12  [[-0.24683, -0.045153, -0.29972, 0.0937, -0.34...  \n",
      "13  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "14  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "15  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "16  [[-0.24834, 0.90814, 0.35553, -0.16831, -0.556...  \n",
      "17  [[-0.36526, 0.48584, -0.19377, -0.13632, 0.484...  \n",
      "18  [[-0.62626, 0.7564, -0.061648, 0.080622, 0.700...  \n",
      "19  [[0.0052971, 0.18642, 0.14117, -0.068971, 0.14...  \n",
      "20  [[-0.064549, -0.13812, 0.50017, 0.41434, 0.458...  \n",
      "21  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
      "22  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
      "23  [[0.061834, 0.55929, 0.2585, 0.27757, 0.18372,...  \n",
      "24  [[-0.0017028, 0.32433, -0.073111, 0.93222, -0....  \n",
      "25  [[-0.82149, 0.84135, -0.13863, 0.42098, -0.537...  \n",
      "26  [[0.0052971, 0.18642, 0.14117, -0.068971, 0.14...  \n",
      "27  [[-0.62479, -0.6503, -1.1002, -0.34666, 0.0225...  \n",
      "28  [[-0.0033138, 0.38946, 0.2635, -0.29199, 0.380...  \n",
      "29  [[-0.27382, -0.2805, 1.0621, 0.28245, 0.31069,...  \n",
      "30  [[-0.36526, 0.48584, -0.19377, -0.13632, 0.484...  \n"
     ]
    }
   ],
   "source": [
    "# Load existing data from CSV\n",
    "try:\n",
    "    df = pd.read_csv('/workspaces/Project-Uchumi/data/raw/articles.csv')\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame(columns=['url', 'article', 'date'])\n",
    "\n",
    "# Language detection for providence of Swahili data\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "df['language'] = df['article'].apply(detect_language)\n",
    "\n",
    "# Define stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "swahili_stopwords = [\n",
    "    \"na\", \"za\", \"kwa\", \"ya\", \"ndani\", \"je\", \"ni\", \"hata\", \"pia\", \"au\", \"wakati\",\n",
    "    \"hivyo\", \"nini\", \"kama\", \"bila\", \"kisha\", \"sasa\", \"yake\", \"yao\", \"hizo\",\n",
    "    \"zao\", \"yao\", \"yenu\", \"yake\", \"zake\", \"lakini\", \"au\", \"nao\", \"wao\",\n",
    "    \"yao\", \"yake\", \"kwenda\", \"kuwa\", \"kuwa\", \"wao\", \"naye\", \"ninyi\",\n",
    "    \"huku\", \"yako\", \"basi\", \"kabla\", \"kutoka\", \"katika\", \"mimi\",\n",
    "    \"yako\", \"kweli\", \"kabisa\", \"hasa\", \"hapo\", \"hata\", \"hivyo\",\n",
    "    \"mbali\", \"mara\", \"zaidi\", \"karibu\", \"kila\", \"mmoja\", \"mwingine\",\n",
    "    \"nyingine\", \"wengine\", \"yoyote\", \"wote\", \"huyo\", \"huo\", \"kwamba\",\n",
    "    \"lakini\", \"mbali\", \"mimi\", \"mmoja\", \"muda\", \"mwenyewe\", \"naam\",\n",
    "    \"pamoja\", \"sana\", \"sasa\", \"sisi\", \"vile\", \"wa\", \"wakati\", \"wake\",\n",
    "    \"wakiwa\", \"wana\", \"wao\", \"watu\", \"wengine\", \"wote\", \"ya\", \"yake\",\n",
    "    \"yangu\", \"yao\", \"yeye\", \"yule\", \"za\", \"zaidi\", \"zake\"\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'text' was the input text\n",
    "# stopwords_removed_text = ' '.join([word for word in text.split() if word.lower() not in swahili_stopwords])\n",
    "\n",
    "\n",
    "# Define stemmer and lemmatizer for nltk.stem\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Negation handling for sentiment analysis\n",
    "negation_words = [\"no\", \"not\", \"never\", \"none\", \"nobody\", \"nowhere\", \"nothing\", \"neither\", \"nor\", \"cannot\", \"can't\", \"won't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\"]\n",
    "\n",
    "# Define the <UNK> token\n",
    "UNK_TOKEN = '<UNK>'\n",
    "UNK_VECTOR = np.zeros_like(word_vectors.get_vector(word_vectors.index_to_key[0]))\n",
    "\n",
    "# Add the <UNK> token to your word vectors\n",
    "word_vectors[UNK_TOKEN] = UNK_VECTOR\n",
    "\n",
    "# Function for preprocessing\n",
    "def process_tokens(text, language):\n",
    "    # Remove special characters, numbers, and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform stemming or lemmatization\n",
    "    if language == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in english_stopwords]\n",
    "    elif language == 'sw':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in swahili_stopwords]\n",
    "\n",
    "    negated = False\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            negated = not negated\n",
    "        elif negated:\n",
    "            processed_tokens.append('NOT_' + token)\n",
    "        else:\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def get_word_embeddings(processed_tokens):\n",
    "    # Get word embeddings for each token\n",
    "    embeddings = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_vectors:\n",
    "            embeddings.append(word_vectors[token])\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            embeddings.append(word_vectors['<UNK>'])  # Use a special token for unknown words\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "    # Reconstruct the text\n",
    "\n",
    "def preprocess_text(text, language):\n",
    "    processed_tokens = process_tokens(text, language)\n",
    "    cleaned_article = ' '.join(processed_tokens)\n",
    "    word_vectors = get_word_embeddings(processed_tokens)\n",
    "\n",
    "    return cleaned_article, word_vectors\n",
    "\n",
    "\n",
    "# Apply preprocessing to each article\n",
    "df['cleaned_article'], df['word_vectors'] = zip(*df.apply(lambda row: preprocess_text(row['article'], row['language']), axis=1))\n",
    "\n",
    "# Print the preprocessed data\n",
    "print(df[['article', 'cleaned_article', 'word_vectors']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821e7fa-40d5-4aa7-9285-8e508e43bfcd",
   "metadata": {},
   "source": [
    "## Emoji removal (Ignore for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9724c2-4989-4e37-a880-c807c84f7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! \n"
     ]
    }
   ],
   "source": [
    "# Emoji Removal for future if applicable in say tweets or comments or Linkedin Posts\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Example usage\n",
    "text_with_emojis = \"Hello! ðŸ˜€ðŸš€ðŸŒŸ\"\n",
    "text_without_emojis = remove_emojis(text_with_emojis)\n",
    "print(text_without_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14eefa-b90a-4673-b383-03599e046d27",
   "metadata": {},
   "source": [
    "## Padding & Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6435288-4876-446c-8378-c5446c04d426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0, -1,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  1,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  1, ...,  0,  1,  0],\n",
       "        [ 0,  1,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the maximum sequence length\n",
    "max_length = 1000\n",
    "# Pad and truncate the sequences\n",
    "padded_sequences = pad_sequences(df['word_vectors'], maxlen=max_length, padding='post', truncating='post')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31f6ac-5659-4c97-9d56-94b7dea43a6d",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a9061d-0ad2-45ae-9ea1-9e0c36fa4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/validation and test sets\n",
    "X_train_val, X_test = train_test_split(padded_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training/validation set into training and validation sets\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b6c282-2e47-43c8-87ca-4ad858750b92",
   "metadata": {},
   "source": [
    "## Export Pre-processed Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a921265a-3648-4bfb-8aa8-8b94a7c3c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/workspaces/Project-Uchumi/data/processed/preprocessed_articles.csv\", index=False, mode='a', header=not os.path.exists(\"/workspaces/Project-Uchumi/data/processed/preprocessed_articles.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
