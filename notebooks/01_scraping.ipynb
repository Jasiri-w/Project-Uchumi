{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trafilatura.sitemaps import sitemap_search\n",
    "from trafilatura import fetch_url, extract, bare_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_sitemap(resource_url: str) -> list:\n",
    "    \"\"\"\n",
    "    Funzione che crea un DataFrame Pandas di URL e articoli.\n",
    "    Function to create a Pandas DataFrame from the article URL\n",
    "    \"\"\"\n",
    "    urls = sitemap_search(resource_url)\n",
    "    print(f'Urls: {urls}')\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_article(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Estrae un articolo da una URL con Trafilatura\n",
    "    Extract an article from a URL from Trafiltura\n",
    "    \"\"\"\n",
    "    downloaded = fetch_url(url)\n",
    "    article = extract(downloaded, favor_precision=True)\n",
    "\n",
    "    return article\n",
    "\n",
    "def extract_date(url: str) -> dict:\n",
    "\n",
    "    downloaded = fetch_url(url)\n",
    "    date = bare_extraction(downloaded,favor_precision=True)['date']\n",
    "\n",
    "    return date\n",
    "\n",
    "def create_dataset(list_of_websites: list, df_original=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Funzione che crea un DataFrame Pandas di URL e articoli.\n",
    "    Function to create a Pandas Dataframe from an article URL\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for website in tqdm(list_of_websites, desc=\"Websites\"):\n",
    "        urls = get_urls_from_sitemap(website)\n",
    "        for url in tqdm(urls, desc=\"URLs\"):\n",
    "            if df_original is None:\n",
    "                if url in df_original['url'].tolist():\n",
    "                    continue\n",
    "            d = {\n",
    "                'url': url,\n",
    "                \"article\": extract_article(url),\n",
    "                \"date\" : pd.to_datetime(extract_date(url))\n",
    "            }\n",
    "            data.append(d)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if df_original:\n",
    "        # Append only rows from df2 that are not in df1\n",
    "        df_original = pd.concat([df_original, df[~df.apply(tuple,1).isin(df_original.apply(tuple,1))]])\n",
    "        df = df_original\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_websites = [\n",
    "    \"https://nation.africa/kenya\",\n",
    "    \"https://www.standardmedia.co.ke/\",\n",
    "    \"https://www.businessdailyafrica.com/\",\n",
    "    \"https://www.pd.co.ke/\",\n",
    "    \"https://www.citizen.digital/\",\n",
    "    \"https://www.nationmedia.com/brands/daily-nation/\",\n",
    "    \"https://www.the-star.co.ke/\"\n",
    "]\n",
    "\n",
    "df = create_dataset(list_of_websites, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public debt relevant terms\n",
    "# relevant_keywords = ['public debt', 'public budget', 'public finance management', 'budget trends', 'budget theft']\n",
    "relevant_keywords = ['debt', 'budget', 'finance', 'trends', 'theft']\n",
    "\n",
    "# Filter relevant blogs\n",
    "df = df[df['article'].str.contains('|'.join(relevant_keywords), case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Dataset to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dataset.csv\", index=False, mode='a', header=not os.path.exists(\"dataset.csv\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
